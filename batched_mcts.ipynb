{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"apple/OpenELM-270M-Instruct\", trust_remote_code=True).to(\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/Dolphin-Llama2-7B-GPTQ\")\n",
    "\n",
    "model.eval()\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_logits(logits, attention_mask):\n",
    "    last_unpadded_indices = attention_mask.sum(dim=1) - 1\n",
    "\n",
    "    filtered_logits = []\n",
    "\n",
    "    for i in range(logits.size(0)):\n",
    "        filtered_logits.append(logits[i, :last_unpadded_indices[i] + 1, :])\n",
    "    \n",
    "    max_length = max(logit.size(0) for logit in filtered_logits)\n",
    "    padded_logits = torch.zeros((logits.size(0), max_length, logits.size(2)), device=logits.device)\n",
    "    \n",
    "    for i, logit in enumerate(filtered_logits):\n",
    "        padded_logits[i, :logit.size(0), :] = logit\n",
    "    \n",
    "    return padded_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, context_ids, attention_mask, unique_token_ids, n=50, temperature=0.1, repetition_penalty=1.1):\n",
    "    with torch.inference_mode(), torch.cuda.amp.autocast():\n",
    "        for _ in range(n):\n",
    "            logits = model(input_ids=context_ids, attention_mask=attention_mask).logits\n",
    "            filtered_logits = filter_logits(logits, attention_mask)\n",
    "\n",
    "            last_logits = filtered_logits[torch.arange(filtered_logits.size(0)), attention_mask.sum(dim=1) - 1, :]\n",
    "\n",
    "            mask = torch.full_like(last_logits, -float('inf'))\n",
    "            mask[:, unique_token_ids] = 0\n",
    "            next_token_logits = last_logits + mask\n",
    "\n",
    "            next_token_probs = F.softmax(next_token_logits / temperature, dim=-1)\n",
    "\n",
    "            for i in range(context_ids.shape[1]):\n",
    "                next_token_probs[:, context_ids[:, i]] /= repetition_penalty\n",
    "\n",
    "            flattened_probs = next_token_probs.view(-1)\n",
    "            max_prob_index = torch.argmax(flattened_probs)\n",
    "\n",
    "            vocab_size = next_token_probs.shape[1]\n",
    "            token_index = max_prob_index % vocab_size\n",
    "\n",
    "            next_token = token_index.unsqueeze(0)\n",
    "\n",
    "            insert_positions = torch.sum(attention_mask, dim=1).unsqueeze(-1)\n",
    "\n",
    "            new_context_ids = []\n",
    "            new_attention_mask = []\n",
    "\n",
    "            for batch_idx in range(context_ids.size(0)):\n",
    "                position = insert_positions[batch_idx].item()\n",
    "                \n",
    "                new_sequence = torch.cat((\n",
    "                    context_ids[batch_idx, :position],\n",
    "                    next_token,\n",
    "                    context_ids[batch_idx, position+1:]\n",
    "                ), dim=0)\n",
    "                new_context_ids.append(new_sequence)\n",
    "                \n",
    "                new_mask = attention_mask[batch_idx].clone()\n",
    "                new_mask[position] = 1\n",
    "                new_attention_mask.append(new_mask)\n",
    "\n",
    "            context_ids = torch.stack(new_context_ids)\n",
    "            attention_mask = torch.stack(new_attention_mask)\n",
    "\n",
    "            if '\"' in tokenizer.decode(next_token):\n",
    "                break\n",
    "\n",
    "    return context_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "needle = '''human said, \"george's birthday is on the 31st of may\". agent replied, \"okay, i'll remember that\".\n",
    "human said, \"when is george's birthday?\" agent replied, \"on the'''\n",
    "\n",
    "haystack = '''human said, \"hello\". agent replied, \"hi, how are you?\".\n",
    "human said, \"when is george's birthday?\" agent replied, \"on the'''\n",
    "\n",
    "context_texts = [haystack] * 99 + [needle]\n",
    "\n",
    "encoded_output = tokenizer(context_texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=200)\n",
    "encoded_output = {k: v.to(\"cuda\") for k, v in encoded_output.items()}\n",
    "\n",
    "context_ids = encoded_output['input_ids']\n",
    "attention_mask = encoded_output['attention_mask']\n",
    "\n",
    "concatenated_ids = torch.cat([context_ids], dim=0)\n",
    "unique_token_ids = list(set(concatenated_ids.view(-1).tolist()))\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human said, \"hello\". agent replied, \"hi, how are you?\".\n",
      "human said, \"when is george's birthday?\" agent replied, \"on the 31st of may\".\n"
     ]
    }
   ],
   "source": [
    "with torch.inference_mode():\n",
    "    output = generate(model, tokenizer, context_ids, attention_mask, unique_token_ids, 25)\n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
